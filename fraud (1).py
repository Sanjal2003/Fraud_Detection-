# -*- coding: utf-8 -*-
"""fraud.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TYeg_FD2VG92I9ROzQFwvNxojMl--0_U
"""

import pandas as pd
import numpy as np

df=pd.read_csv('/content/creditcard.csv')
df

df.shape

df.columns

df.info()

df.isna().sum()

df.dropna(axis=0,inplace=True)

df.shape

df['Class'] = df['Class'].astype(int)
df.info()

df['Class'].value_counts()

import matplotlib.pyplot as plt

df['Class'].value_counts().plot(kind='bar')
plt.title('Class Value Counts')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks([0, 1], ['Non Fraud', 'Fraud'],rotation=0)
plt.show()

# Convert Time to hours for easier visualization
df['Hour'] = df['Time'] / 3600

# Separate fraudulent and non-fraudulent transactions
fraud_df = df[df['Class'] == 1]
non_fraud_df = df[df['Class'] == 0]

# 1. Compare summary statistics of numerical features
print("Fraudulent transactions summary statistics:")
fraud_df.describe()

print("\nNon-fraudulent transactions summary statistics:")
non_fraud_df.describe()

# 2. Visualize distributions of some features for both classes
features_to_compare = ['V1', 'Time', 'Amount']

for feature in features_to_compare:
  plt.figure(figsize=(10, 6))
  plt.hist(non_fraud_df[feature], bins=50, alpha=0.5, label='Non-Fraud')
  plt.hist(fraud_df[feature], bins=50, alpha=0.5, label='Fraud')
  plt.title(f'Distribution of {feature} by Class')
  plt.xlabel(feature)
  plt.ylabel('Frequency')
  plt.legend()
  plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate correlation matrices
fraud_corr = fraud_df.corr()
non_fraud_corr = non_fraud_df.corr()

# Create subplots
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

# Plot heatmap for fraudulent transactions
sns.heatmap(fraud_corr, ax=axes[0], annot=False, cmap='coolwarm')
axes[0].set_title('Correlation Matrix for Fraudulent Transactions')

# Plot heatmap for non-fraudulent transactions
sns.heatmap(non_fraud_corr, ax=axes[1], annot=False, cmap='coolwarm')
axes[1].set_title('Correlation Matrix for Non-Fraudulent Transactions')

plt.tight_layout()
plt.show()

# 4. Consider time-based patterns

plt.figure(figsize=(12, 6))
non_fraud_df['Hour'].hist(bins=24, alpha=0.5, label='Non-Fraud')
fraud_df['Hour'].hist(bins=24, alpha=0.5, label='Fraud')
plt.title('Number of Transactions by Hour of Day')
plt.xlabel('Hour')
plt.ylabel('Count')
plt.legend()
plt.show()

# Scatter plot of Time vs. Amount
plt.figure(figsize=(10, 4))
plt.scatter(non_fraud_df['Time'], non_fraud_df['Amount'], alpha=0.5, label='Non-Fraud', s=1)
plt.scatter(fraud_df['Time'], fraud_df['Amount'], alpha=0.5, label='Fraud', s=10) # Use a larger size for fraud points
plt.title('Transaction Amount vs. Time')
plt.xlabel('Time (seconds)')
plt.ylabel('Amount')
plt.legend()
plt.show()

# Time-based features
df['Hour'] = np.floor(df['Time'] / 3600) % 24
df['TimeBin'] = pd.cut(df['Hour'], bins=[-1, 6, 12, 18, 24], labels=['Night', 'Morning', 'Afternoon', 'Evening'])
df['IsNight'] = df['Hour'].apply(lambda x: 1 if x < 6 or x >= 22 else 0)
df['DeltaTimePrev'] = df['Time'].diff().fillna(0)

# Amount-based features
df['LogAmount'] = np.log1p(df['Amount'])
df['HighAmountFlag'] = df['Amount'] > df['Amount'].quantile(0.99)
df['AmountBin'] = pd.cut(df['Amount'], bins=[-1, 10, 100, 1000, df['Amount'].max()], labels=['Low', 'Medium', 'High', 'Very High'])
df['ZScoreAmount'] = (df['Amount'] - df['Amount'].mean()) / df['Amount'].std()

# Behavior-based features (rolling features)
df['TxnCountPastHour'] = df['Time'].rolling(window=100, min_periods=1).apply(lambda x: sum(np.diff(x) <= 3600))
df['CumulativeTxnAmount'] = df['Amount'].cumsum()

df

from sklearn.preprocessing import LabelEncoder

# Select categorical columns
categorical_cols = ['TimeBin', 'AmountBin']

# Apply LabelEncoder to each categorical column
label_encoders = {}
for col in categorical_cols:
  label_encoders[col] = LabelEncoder()
  df[col] = label_encoders[col].fit_transform(df[col])

df

from sklearn.preprocessing import StandardScaler

# Select numerical features for scaling
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
# Exclude the target variable if it's included in numerical_cols
if 'Class' in numerical_cols:
    numerical_cols.remove('Class')

# Initialize the StandardScaler
scaler = StandardScaler()

# Apply scaling to the selected numerical columns
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

df.head()

# Separate features (X) and target (y)
X = df.drop('Class', axis=1)
y = df['Class']

from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the training data
X_resampled, y_resampled = smote.fit_resample(X, y)

# Print the new class distribution
print("\nClass value counts after SMOTE:")
print(pd.Series(y_resampled).value_counts())

# Visualize the new class distribution
pd.Series(y_resampled).value_counts().plot(kind='bar')
plt.title('Class Value Counts After SMOTE')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks([0, 1], ['Non Fraud', 'Fraud'],rotation=0)
plt.show()



from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
logreg=LogisticRegression()
logreg.fit(X_train,y_train)

log_pred = logreg.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Calculate accuracy
accuracy = accuracy_score(y_test, log_pred)
print(f"Accuracy: {accuracy:.2f}")

log_classification_report = classification_report(y_test, log_pred)
print("Classification Report:")
print(log_classification_report)

confusion_mat = confusion_matrix(y_test, log_pred)
print("Confusion Matrix:")
print(confusion_mat)

# Visualize the Confusion Matrix as a Heatmap
plt.figure(figsize=(4, 4))
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Non-Fraud', 'Predicted Fraud'],
            yticklabels=['Actual Non-Fraud', 'Actual Fraud'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()



from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
dt.fit(X_train,y_train)

dt_pred=dt.predict(X_test)

dt_classification_report = classification_report(y_test,dt_pred)
print("Classification Report (Decision Tree):")
print(dt_classification_report)

dt_confusion_mat = confusion_matrix(y_test, dt_pred)
print("Confusion Matrix:")
print(confusion_mat)

# Visualize the Confusion Matrix as a Heatmap
plt.figure(figsize=(4, 4))
sns.heatmap(dt_confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Non-Fraud', 'Predicted Fraud'],
            yticklabels=['Actual Non-Fraud', 'Actual Fraud'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

import xgboost as xgb
xgb=xgb.XGBClassifier()
xgb.fit(X_train,y_train)

xgb_pred=xgb.predict(X_test)

xgb_classification_report = classification_report(y_test,xgb_pred)
print("Classification Report (XGBoost):")
print(xgb_classification_report)

xgb_confusion_mat = confusion_matrix(y_test, xgb_pred)
print("Confusion Matrix:")
print(confusion_mat)

# Visualize the Confusion Matrix as a Heatmap
plt.figure(figsize=(4, 4))
sns.heatmap(xgb_confusion_mat, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Non-Fraud', 'Predicted Fraud'],
            yticklabels=['Actual Non-Fraud', 'Actual Fraud'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()



from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate evaluation metrics for each model
log_precision = precision_score(y_test, log_pred)
log_recall = recall_score(y_test, log_pred)
log_f1 = f1_score(y_test, log_pred)

dt_precision = precision_score(y_test, dt_pred)
dt_recall = recall_score(y_test, dt_pred)
dt_f1 = f1_score(y_test, dt_pred)

xgb_precision = precision_score(y_test, xgb_pred)
xgb_recall = recall_score(y_test, xgb_pred)
xgb_f1 = f1_score(y_test, xgb_pred)

# Create a dictionary of metrics
metrics = {
    'Logistic Regression': {'Precision': log_precision, 'Recall': log_recall, 'F1-Score': log_f1},
    'Decision Tree': {'Precision': dt_precision, 'Recall': dt_recall, 'F1-Score': dt_f1},
    'XGBoost': {'Precision': xgb_precision, 'Recall': xgb_recall, 'F1-Score': xgb_f1}
}

# Convert the dictionary to a DataFrame for easier plotting
metrics_df = pd.DataFrame(metrics).T

# Plotting the metrics
fig, axes = plt.subplots(1, 3, figsize=(22, 6), sharey=True)

metrics_df['Precision'].plot(kind='bar', ax=axes[0], color='skyblue')
axes[0].set_title('Precision Comparison')
axes[0].set_ylabel('Score')
axes[0].tick_params(axis='x', rotation=0)

metrics_df['Recall'].plot(kind='bar', ax=axes[1], color='salmon')
axes[1].set_title('Recall Comparison')
axes[1].tick_params(axis='x', rotation=0)

metrics_df['F1-Score'].plot(kind='bar', ax=axes[2], color='lightgreen')
axes[2].set_title('F1-Score Comparison')
axes[2].tick_params(axis='x', rotation=0)

plt.tight_layout()
plt.show()

importance = xgb.feature_importances_

# Create a DataFrame to display feature importance
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

print("\nFeature Importance (XGBoost):")
print(feature_importance_df)

# Visualize the top N most important features
plt.figure(figsize=(10, 5))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(12), palette='viridis') # Adjust head() to show more/fewer features
plt.title('Top 10 Most Important Features (XGBoost)')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()



# Get the top 11 feature names from the feature importance DataFrame
top_11_features = feature_importance_df['Feature'].head(11).tolist()

# Create a new DataFrame with only the top 11 features and the target variable
df_top_11 = df[top_11_features + ['Class']]

df_top_11

# Separate features (X) and target (y) for the new DataFrame
X_top_11 = df_top_11.drop('Class', axis=1)
y_top_11 = df_top_11['Class']

# Apply SMOTE to the data with top 11 features
smote_top_11 = SMOTE(random_state=42)
X_resampled_top_11, y_resampled_top_11 = smote_top_11.fit_resample(X_top_11, y_top_11)

# Split the resampled data into training and testing sets
X_train_top_11, X_test_top_11, y_train_top_11, y_test_top_11 = train_test_split(
    X_resampled_top_11, y_resampled_top_11, test_size=0.2, random_state=42)

# Initialize and train the XGBoost model
import xgboost as xgb
xgb_top_11 = xgb.XGBClassifier()
xgb_top_11.fit(X_train_top_11, y_train_top_11)

# Make predictions
xgb_pred_top_11 = xgb_top_11.predict(X_test_top_11)

# Evaluate the model
xgb_classification_report_top_11 = classification_report(y_test_top_11, xgb_pred_top_11)
print("Classification Report (XGBoost with Top 11 Features):")
print(xgb_classification_report_top_11)

xgb_confusion_mat_top_11 = confusion_matrix(y_test_top_11, xgb_pred_top_11)
print("Confusion Matrix (XGBoost with Top 11 Features):")
print(xgb_confusion_mat_top_11)

# Visualize the Confusion Matrix as a Heatmap
plt.figure(figsize=(4, 4))
sns.heatmap(xgb_confusion_mat_top_11, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Non-Fraud', 'Predicted Fraud'],
            yticklabels=['Actual Non-Fraud', 'Actual Fraud'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (XGBoost with Top 11 Features)')
plt.show()

from sklearn.metrics import roc_curve, auc
# Get probability predictions for the positive class
y_pred_proba_top_11 = xgb_top_11.predict_proba(X_test_top_11)[:, 1]

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test_top_11, y_pred_proba_top_11)

# Calculate the AUC (Area Under the Curve)
roc_auc = auc(fpr, tpr)

print(f"AUC for XGBoost with Top 11 Features: {roc_auc:.4f}")

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.4f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()